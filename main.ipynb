{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connecting bedrock runtime with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\n",
    "    \tservice_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assigning model Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amazon.titan-text-express-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providing payload, to define the model behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"maxTokenCount\": 200,\n",
    "            \"temperature\": 0.9,\n",
    "            \"topP\": 1  # ✅ Keep only topP\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model invokation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_client.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '0d772eb0-c4a6-491f-9ba1-a9910a966bd9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 07 Mar 2025 05:33:11 GMT', 'content-type': 'application/json', 'content-length': '355', 'connection': 'keep-alive', 'x-amzn-requestid': '0d772eb0-c4a6-491f-9ba1-a9910a966bd9', 'x-amzn-bedrock-invocation-latency': '3061', 'x-amzn-bedrock-output-token-count': '55', 'x-amzn-bedrock-input-token-count': '6'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': <botocore.response.StreamingBody object at 0x0000023947CF2590>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting response from the bedrock and printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "The required content is based on the model's best judgment, which may be based on user terms, query interpretation, and other inputs that may or may not be present in the provided content. If you have any questions, please do not hesitate to let me know.\n"
     ]
    }
   ],
   "source": [
    "result = json.loads(response[\"body\"].read())\n",
    "generated_text = result[\"results\"][0][\"outputText\"]\n",
    "    \n",
    "print(f\"Response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# changing Prompt to see model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "``` C++\n",
      "#include <iostream>\n",
      "using namespace std;\n",
      "void bubbleSort(int arr[], int n)\n",
      "{\n",
      "    int temp;\n",
      "    for (int i = 0; i < n - 1; i++)\n",
      "    {\n",
      "        for (int j = 0; j < n - i - 1; j++)\n",
      "        {\n",
      "            if (arr[j] > arr[j + 1])\n",
      "            {\n",
      "                temp = arr[j];\n",
      "                arr[j] = arr[j + 1];\n",
      "                arr[j + 1] = temp;\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "int main()\n",
      "{\n",
      "    int arr[] = {10, 20, 30, 40, 50};\n",
      "    int n = sizeof(arr) / sizeof(arr[0]);\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "prompt = \"write me code to perform bubble sort algorithm in c++ language?\"\n",
    "payload = {\n",
    "        \"inputText\": prompt,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"maxTokenCount\": 200,\n",
    "            \"temperature\": 0.1,\n",
    "            \"topP\": 1  # ✅ Keep only topP\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"body\"].read())\n",
    "generated_text = result[\"results\"][0][\"outputText\"]\n",
    "    \n",
    "print(f\"Response: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same Prompt using differnet model to campare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Sorry - this model is unable to respond to this request.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"amazon.titan-text-lite-v1\"\n",
    "prompt = \"write me code to perform bubble sort algorithm in c++ language?\"\n",
    "payload = {\n",
    "    \"inputText\": prompt,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"temperature\": 0,  \n",
    "    }\n",
    "}\n",
    "\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=model_id,\n",
    "    body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "result = json.loads(response[\"body\"].read())\n",
    "generated_text = result[\"results\"][0][\"outputText\"]\n",
    "    \n",
    "print(f\"Response: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
